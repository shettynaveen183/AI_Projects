{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"your_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine= \"davinci-002\",\n",
    "        prompt= prompt,\n",
    "        max_tokens=10,\n",
    "        temperature= 0.7\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"your-api-key-here\"  # Replace with your actual OpenAI API key\n",
    "\n",
    "def chat_with_gpt(prompt):\n",
    "    try:\n",
    "        # Generate a response from the model\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # You can use \"gpt-4\" if you have access\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract and return the assistant's response\n",
    "        return response['choices'][0]['message']['content']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example prompt\n",
    "user_input = \"What is the capital of France?\"\n",
    "\n",
    "# Chat with GPT\n",
    "response = chat_with_gpt(user_input)\n",
    "\n",
    "# Print the response\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"\"  # Replace with your actual OpenAI API key\n",
    "\n",
    "def chat_with_gpt(prompt):\n",
    "    try:\n",
    "        # Generate a response using the completions endpoint\n",
    "        response = openai.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # You can use \"gpt-4\" if you have access\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract and return the assistant's response\n",
    "        return response['choices'][0]['message']['content']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example prompt\n",
    "user_input = \"What is the capital of France?\"\n",
    "\n",
    "# Chat with GPT\n",
    "response = chat_with_gpt(user_input)\n",
    "\n",
    "# Print the response\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"your_key\"  # Replace with your actual OpenAI API key\n",
    "\n",
    "def chat_with_gpt(prompt):\n",
    "    try:\n",
    "        # Use prompt-based completion for davinci-002\n",
    "        response = openai.completions.create(\n",
    "            model=\"text-davinci-002\",  # Change this if you're using another model\n",
    "            prompt=prompt,\n",
    "            max_tokens=100,  # Limit the response length if necessary\n",
    "            temperature=0.7  # Control randomness\n",
    "        )\n",
    "        \n",
    "        # Extract and return the assistant's response\n",
    "        return response['choices'][0]['text'].strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example prompt\n",
    "user_input = \"What is the capital of France?\"\n",
    "\n",
    "# Chat with GPT\n",
    "response = chat_with_gpt(user_input)\n",
    "\n",
    "# Print the response\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"your_key\"  # Replace with your actual OpenAI API key\n",
    "\n",
    "def chat_with_gpt(messages):\n",
    "    try:\n",
    "        # Use message-based completion for chat models like gpt-3.5-turbo or gpt-4\n",
    "        response = openai.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # You can use \"gpt-4\" if you have access\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract and return the assistant's response\n",
    "        return response['choices'][0]['message']['content']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "# Chat with GPT\n",
    "response = chat_with_gpt(conversation)\n",
    "\n",
    "# Print the response\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"your_key\"  # Replace with your actual OpenAI API key\n",
    "\n",
    "def generate_text(prompt):\n",
    "    try:\n",
    "        # Make sure to pass both 'model' and 'prompt' arguments\n",
    "        response = openai.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Replace with your chosen model, e.g., gpt-3.5-turbo or gpt-4\n",
    "            prompt=prompt,\n",
    "            max_tokens=100,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Return the generated text\n",
    "        return response.choices[0].text.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example prompt\n",
    "user_input = \"What is the capital of France?\"\n",
    "\n",
    "# Generate response\n",
    "response = generate_text(user_input)\n",
    "\n",
    "# Print the response\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i wont get output because my free-tier has been ended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customizing GPT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_tokens):\n",
    "    response = openai.Completion.create(\n",
    "        engine= 'davinci-002',\n",
    "        prompt= prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature= 0.7\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = generate_text(prompt, 5)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poetic_chatbot(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\":\"You are a poetic chatbot.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"When is Google founded?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\":\"Tn the late 90s\"\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":\"Which country has youngest president?\"\n",
    "                \n",
    "            },\n",
    "           {\n",
    "             \"role\": \"assistant\",\n",
    "             \"content\": \"Australia\"\n",
    "                   \n",
    "           },\n",
    "           {\n",
    "               \"role\":\"user\",\n",
    "               \"content\": prompt\n",
    "           }\n",
    "        ],\n",
    "        temperature = 1,\n",
    "        max_tokens = 256\n",
    "        \n",
    "    )\n",
    "    return response.choice[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"when was cheese first made?\"\n",
    "poetic_chatbot(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"pydantic<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall langchain pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install langchain pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebBaseLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n",
      "File \u001b[1;32m<stringsource>:69\u001b[0m, in \u001b[0;36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1429\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._line_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1471\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1278\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1906\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2197\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2194\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2197\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2199\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2202\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2266\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2263\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[0;32m   2264\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[1;32m-> 2266\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2267\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m   2269\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shett\\anaconda3\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\shett\\anaconda3\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
